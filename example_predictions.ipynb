{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "video_path = \"data/example/input_video.mp4\"\n",
    "model = \"yolo\"\n",
    "interesting_frame = 210\n",
    "\n",
    "images_for_paper_path = \"data/example/images_for_paper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Football Referee Evaluation API is running'}\n"
     ]
    }
   ],
   "source": [
    "# Test if service is running\n",
    "response = requests.get(\"http://localhost:4321/\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_post_request(endpoint: str, body: dict) -> dict:\n",
    "    response = requests.post(f\"http://localhost:4321/{endpoint}\", json=body)\n",
    "    if response.status_code != 200:\n",
    "        msg = f\"Request failed with status code {response.status_code}: {response.text}\"\n",
    "        raise Exception(msg)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track players\n",
    "tracking_results_path = \"data/example/predictions/tracking_results\"\n",
    "tracking_results = make_post_request(\n",
    "    endpoint=\"track\", \n",
    "    body = {\n",
    "        \"video_path\": video_path,\n",
    "        \"model\": model,\n",
    "        \"results_path\": tracking_results_path\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detections file\n",
    "import json\n",
    "with open(os.path.join(tracking_results_path, \"detections.json\"), \"r\") as f:\n",
    "    detections = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved annotated frame to data/example/images_for_paper/tracking.png\n"
     ]
    }
   ],
   "source": [
    "# Load the video and extract the interesting frame\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Set the frame position to the interesting frame\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, interesting_frame)\n",
    "\n",
    "# Read the frame\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    raise Exception(f\"Could not read frame {interesting_frame}\")\n",
    "\n",
    "# Get detections for this specific frame\n",
    "frame_detections = next((item for item in detections if item[\"frame_id\"] == interesting_frame), None)\n",
    "\n",
    "if frame_detections is None:\n",
    "    print(f\"No detections found for frame {interesting_frame}\")    \n",
    "\n",
    "# Create a copy of the frame to draw on\n",
    "annotated_frame = frame.copy()\n",
    "\n",
    "# Draw bounding boxes and IDs\n",
    "for detection in frame_detections[\"detections\"]:\n",
    "    # Extract bbox coordinates\n",
    "    bbox = detection[\"bbox\"]\n",
    "    x1, y1, x2, y2 = bbox[\"x1\"], bbox[\"y1\"], bbox[\"x2\"], bbox[\"y2\"]\n",
    "    track_id = detection[\"track_id\"]\n",
    "    \n",
    "    # Draw bright green bbox\n",
    "    color = (0, 255, 0)  # Bright green in BGR\n",
    "    thickness = 2\n",
    "    cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness)\n",
    "    \n",
    "    # Add track ID text\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(annotated_frame, f\"{track_id}\", (int(x1), int(y1) - 10), \n",
    "                font, 0.7, color, 2)\n",
    "    \n",
    "    # Draw a black circle at the player's feet (bottom center of bounding box)\n",
    "    feet_x = int((x1 + x2) / 2)  # Center x-coordinate\n",
    "    feet_y = int(y2)  # Bottom y-coordinate\n",
    "    circle_color = (0, 0, 0)  # Black in BGR\n",
    "    circle_radius = 12\n",
    "    circle_thickness = -1  # Filled circle\n",
    "    cv2.circle(annotated_frame, (feet_x, feet_y), circle_radius, circle_color, circle_thickness)\n",
    "\n",
    "# Save the annotated frame as a PNG image\n",
    "os.makedirs(images_for_paper_path, exist_ok=True)\n",
    "output_path = f\"{images_for_paper_path}/tracking.png\"\n",
    "cv2.imwrite(output_path, annotated_frame)\n",
    "\n",
    "print(f\"Saved annotated frame to {output_path}\")\n",
    "\n",
    "# Release the video capture\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track players\n",
    "transform_coordinates_results_path = \"data/example/predictions/coordinate_transformation\"\n",
    "transform_coordinates_results = make_post_request(\n",
    "    endpoint=\"transform-coordinates\", \n",
    "    body = {\n",
    "        \"video_path\": video_path,\n",
    "        \"detections\": detections,\n",
    "        \"results_path\": transform_coordinates_results_path\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detections file\n",
    "import json\n",
    "with open(os.path.join(transform_coordinates_results_path, \"detections.json\"), \"r\") as f:\n",
    "    detections = json.load(f)\n",
    "    \n",
    "warped_image_path = f\"data/example/predictions/coordinate_transformation/warped_images/frame_{interesting_frame:06d}.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved annotated warped image to data/example/images_for_paper/warped_tracking.png\n"
     ]
    }
   ],
   "source": [
    "# Load the warped image and the pitch background\n",
    "warped_image = cv2.imread(warped_image_path)\n",
    "pitch_background = cv2.imread(\"src/utils/pitch_2.png\")\n",
    "\n",
    "if warped_image is None:\n",
    "    print(f\"Error: Could not load warped image from {warped_image_path}\")\n",
    "elif pitch_background is None:\n",
    "    print(f\"Error: Could not load pitch background from pitch_2.png\")\n",
    "else:\n",
    "    # Get pitch background dimensions\n",
    "    pitch_height, pitch_width = pitch_background.shape[:2]\n",
    "    \n",
    "    # Resize warped image to match pitch background dimensions\n",
    "    resized_warped_image = cv2.resize(warped_image, (pitch_width, pitch_height))\n",
    "    \n",
    "    # Create a semi-transparent overlay\n",
    "    alpha = 0.6  # Transparency factor (0.0 = fully transparent, 1.0 = fully opaque)\n",
    "    overlay = pitch_background.copy()\n",
    "    \n",
    "    # Overlay the warped image on the pitch background\n",
    "    cv2.addWeighted(resized_warped_image, alpha, pitch_background, 1 - alpha, 0, overlay)\n",
    "    \n",
    "    # Create a copy for annotation\n",
    "    annotated_image = overlay.copy()\n",
    "    \n",
    "    # Find the detections for the interesting frame\n",
    "    frame_detections = None\n",
    "    for frame_data in detections:\n",
    "        if frame_data[\"frame_id\"] == interesting_frame:\n",
    "            frame_detections = frame_data[\"detections\"]\n",
    "            break\n",
    "    \n",
    "    if frame_detections:\n",
    "        # Draw black dots at player positions\n",
    "        for detection in frame_detections:\n",
    "            if \"minimap_coordinates\" in detection and detection[\"minimap_coordinates\"]:\n",
    "                coords = detection[\"minimap_coordinates\"]\n",
    "                \n",
    "                # Rescale coordinates using x_max and y_max to match pitch dimensions\n",
    "                x_max = coords[\"x_max\"]\n",
    "                y_max = coords[\"y_max\"]\n",
    "                x_scaled = int((coords[\"x\"] / x_max) * pitch_width)\n",
    "                y_scaled = int((coords[\"y\"] / y_max) * pitch_height)\n",
    "                \n",
    "                # Draw a filled black circle (dot)\n",
    "                cv2.circle(annotated_image, (x_scaled, y_scaled), 10, (0, 0, 0), -1)  # -1 means filled circle\n",
    "                \n",
    "    # Save the annotated image\n",
    "    warped_output_path = f\"{images_for_paper_path}/warped_tracking.png\"\n",
    "    cv2.imwrite(warped_output_path, annotated_image)\n",
    "    print(f\"Saved annotated warped image to {warped_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign colors to players\n",
    "color_assignment_results_path = \"data/example/predictions/color_assignment\"\n",
    "color_assignment_results = make_post_request(\n",
    "    endpoint=\"assign-colors\", \n",
    "    body = {\n",
    "        \"video_path\": video_path,\n",
    "        \"detections\": detections,\n",
    "        \"results_path\": color_assignment_results_path\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(color_assignment_results_path, \"detections.json\"), \"r\") as f:\n",
    "    detections = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign roles\n",
    "role_assignment_results_path = \"data/example/predictions/role_assignment\"\n",
    "role_assignment_results = make_post_request(\n",
    "    endpoint=\"assign-roles\", \n",
    "    body = {\n",
    "        \"video_path\": video_path,\n",
    "        \"detections\": detections,\n",
    "        \"results_path\": role_assignment_results_path\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
