{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "video_path = \"data/example/input_video.mp4\"\n",
    "model = \"yolo\"\n",
    "interesting_frames = [212, 400, 460]\n",
    "\n",
    "images_for_paper_path = \"data/example/images_for_paper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Football Referee Evaluation API is running'}\n"
     ]
    }
   ],
   "source": [
    "# Test if service is running\n",
    "response = requests.get(\"http://localhost:4321/\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_post_request(endpoint: str, body: dict) -> dict:\n",
    "    response = requests.post(f\"http://localhost:4321/{endpoint}\", json=body)\n",
    "    if response.status_code != 200:\n",
    "        msg = f\"Request failed with status code {response.status_code}: {response.text}\"\n",
    "        raise Exception(msg)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track players\n",
    "tracking_results_path = \"data/example/predictions/tracking_results\"\n",
    "# tracking_results = make_post_request(\n",
    "#     endpoint=\"track\", \n",
    "#     body = {\n",
    "#         \"video_path\": video_path,\n",
    "#         \"model\": model,\n",
    "#         \"results_path\": tracking_results_path\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detections file\n",
    "import json\n",
    "with open(os.path.join(tracking_results_path, \"detections.json\"), \"r\") as f:\n",
    "    detections = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved annotated frame 212 to data/example/images_for_paper/tracking_frame_212.png\n",
      "Saved annotated frame 400 to data/example/images_for_paper/tracking_frame_400.png\n",
      "Saved annotated frame 460 to data/example/images_for_paper/tracking_frame_460.png\n"
     ]
    }
   ],
   "source": [
    "# Load the video and extract the interesting frames\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(images_for_paper_path, exist_ok=True)\n",
    "\n",
    "# Process each interesting frame\n",
    "for interesting_frame in interesting_frames:\n",
    "    # Set the frame position to the interesting frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, interesting_frame)\n",
    "    \n",
    "    # Read the frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        raise Exception(f\"Could not read frame {interesting_frame}\")\n",
    "    \n",
    "    # Get detections for this specific frame\n",
    "    frame_detections = next((item for item in detections if item[\"frame_id\"] == interesting_frame), None)\n",
    "    \n",
    "    if frame_detections is None:\n",
    "        print(f\"No detections found for frame {interesting_frame}\")\n",
    "        continue\n",
    "    \n",
    "    # Create a copy of the frame to draw on\n",
    "    annotated_frame = frame.copy()\n",
    "    \n",
    "    # Draw bounding boxes and IDs\n",
    "    for detection in frame_detections[\"detections\"]:\n",
    "        # Extract bbox coordinates\n",
    "        bbox = detection[\"bbox\"]\n",
    "        x1, y1, x2, y2 = bbox[\"x1\"], bbox[\"y1\"], bbox[\"x2\"], bbox[\"y2\"]\n",
    "        track_id = detection[\"track_id\"]\n",
    "        \n",
    "        # Draw bright green bbox\n",
    "        color = (0, 255, 0)  # Bright green in BGR\n",
    "        thickness = 2\n",
    "        cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness)\n",
    "        \n",
    "        # Add track ID text\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(annotated_frame, f\"{track_id}\", (int(x1), int(y1) - 10), \n",
    "                    font, 0.7, color, 2)\n",
    "        \n",
    "        # Draw a black circle at the player's feet (bottom center of bounding box)\n",
    "        feet_x = int((x1 + x2) / 2)  # Center x-coordinate\n",
    "        feet_y = int(y2)  # Bottom y-coordinate\n",
    "        circle_color = (0, 0, 0)  # Black in BGR\n",
    "        circle_radius = 12\n",
    "        circle_thickness = -1  # Filled circle\n",
    "        cv2.circle(annotated_frame, (feet_x, feet_y), circle_radius, circle_color, circle_thickness)\n",
    "    \n",
    "    # Save the annotated frame as a PNG image\n",
    "    output_path = f\"{images_for_paper_path}/tracking_frame_{interesting_frame}.png\"\n",
    "    cv2.imwrite(output_path, annotated_frame)\n",
    "    \n",
    "    print(f\"Saved annotated frame {interesting_frame} to {output_path}\")\n",
    "\n",
    "# Release the video capture\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perspective transformation\n",
    "transform_coordinates_results_path = \"data/example/predictions/coordinate_transformation\"\n",
    "# transform_coordinates_results = make_post_request(\n",
    "#     endpoint=\"transform-coordinates\", \n",
    "#     body = {\n",
    "#         \"video_path\": video_path,\n",
    "#         \"detections\": detections,\n",
    "#         \"results_path\": transform_coordinates_results_path\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detections file\n",
    "import json\n",
    "with open(os.path.join(transform_coordinates_results_path, \"detections.json\"), \"r\") as f:\n",
    "    detections = json.load(f)\n",
    "    \n",
    "warped_image_paths = [f\"data/example/predictions/coordinate_transformation/warped_images/frame_{interesting_frame:06d}.jpg\" for interesting_frame in interesting_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved annotated warped image to data/example/images_for_paper/warped_tracking_000212.png\n",
      "Saved annotated warped image to data/example/images_for_paper/warped_tracking_000400.png\n",
      "Saved annotated warped image to data/example/images_for_paper/warped_tracking_000460.png\n"
     ]
    }
   ],
   "source": [
    "# Load the warped images and the pitch background\n",
    "pitch_background = cv2.imread(\"src/utils/pitch_2.png\")\n",
    "\n",
    "if pitch_background is None:\n",
    "    print(f\"Error: Could not load pitch background from pitch_2.png\")\n",
    "else:\n",
    "    # Get pitch background dimensions\n",
    "    pitch_height, pitch_width = pitch_background.shape[:2]\n",
    "    \n",
    "    for i, warped_image_path in enumerate(warped_image_paths):\n",
    "        warped_image = cv2.imread(warped_image_path)\n",
    "        \n",
    "        if warped_image is None:\n",
    "            print(f\"Error: Could not load warped image from {warped_image_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Resize warped image to match pitch background dimensions\n",
    "        resized_warped_image = cv2.resize(warped_image, (pitch_width, pitch_height))\n",
    "        \n",
    "        # Create a semi-transparent overlay\n",
    "        alpha = 0.6  # Transparency factor (0.0 = fully transparent, 1.0 = fully opaque)\n",
    "        overlay = pitch_background.copy()\n",
    "        \n",
    "        # Overlay the warped image on the pitch background\n",
    "        cv2.addWeighted(resized_warped_image, alpha, pitch_background, 1 - alpha, 0, overlay)\n",
    "        \n",
    "        # Create a copy for annotation\n",
    "        annotated_image = overlay.copy()\n",
    "        \n",
    "        # Find the detections for the current interesting frame\n",
    "        current_frame = interesting_frames[i]\n",
    "        frame_detections = None\n",
    "        for frame_data in detections:\n",
    "            if frame_data[\"frame_id\"] == current_frame:\n",
    "                frame_detections = frame_data[\"detections\"]\n",
    "                break\n",
    "        \n",
    "        if frame_detections:\n",
    "            # Draw black dots at player positions\n",
    "            for detection in frame_detections:\n",
    "                if \"minimap_coordinates\" in detection and detection[\"minimap_coordinates\"]:\n",
    "                    coords = detection[\"minimap_coordinates\"]\n",
    "                    \n",
    "                    # Rescale coordinates using x_max and y_max to match pitch dimensions\n",
    "                    x_max = coords[\"x_max\"]\n",
    "                    y_max = coords[\"y_max\"]\n",
    "                    x_scaled = int((coords[\"x\"] / x_max) * pitch_width)\n",
    "                    y_scaled = int((coords[\"y\"] / y_max) * pitch_height)\n",
    "                    \n",
    "                    # Draw a filled black circle (dot)\n",
    "                    cv2.circle(annotated_image, (x_scaled, y_scaled), 10, (0, 0, 0), -1)  # -1 means filled circle\n",
    "                    \n",
    "        # Save the annotated image\n",
    "        warped_output_path = f\"{images_for_paper_path}/warped_tracking_{current_frame:06d}.png\"\n",
    "        cv2.imwrite(warped_output_path, annotated_image)\n",
    "        print(f\"Saved annotated warped image to {warped_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign colors to players\n",
    "color_assignment_results_path = \"data/example/predictions/color_assignment\"\n",
    "# color_assignment_results = make_post_request(\n",
    "#     endpoint=\"assign-colors\", \n",
    "#     body = {\n",
    "#         \"video_path\": video_path,\n",
    "#         \"detections\": detections,\n",
    "#         \"results_path\": color_assignment_results_path\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(color_assignment_results_path, \"detections.json\"), \"r\") as f:\n",
    "    detections = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign roles\n",
    "role_assignment_results_path = \"data/example/predictions/role_assignment\"\n",
    "# role_assignment_results = make_post_request(\n",
    "#     endpoint=\"assign-roles\", \n",
    "#     body = {\n",
    "#         \"video_path\": video_path,\n",
    "#         \"detections\": detections,\n",
    "#         \"results_path\": role_assignment_results_path\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(role_assignment_results_path, \"detections.json\"), \"r\") as f:\n",
    "    detections = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved minimap visualization for frame 212 to data/example/images_for_paper/minimap_000212.png\n",
      "Saved minimap visualization for frame 400 to data/example/images_for_paper/minimap_000400.png\n",
      "Saved minimap visualization for frame 460 to data/example/images_for_paper/minimap_000460.png\n"
     ]
    }
   ],
   "source": [
    "# Visualize players on the pitch with team jersey colors and roles for all interesting frames\n",
    "import numpy as np\n",
    "\n",
    "# Load the soccer pitch\n",
    "pitch_img_original = cv2.imread(\"src/utils/pitch_2.png\")\n",
    "\n",
    "# Get pitch dimensions\n",
    "pitch_height, pitch_width, _ = pitch_img_original.shape\n",
    "\n",
    "# Scale up the pitch image by a factor of 4\n",
    "pitch_img_original = cv2.resize(pitch_img_original, (pitch_width * 4, pitch_height * 4))\n",
    "\n",
    "# Get updated pitch dimensions after scaling\n",
    "pitch_height, pitch_width, _ = pitch_img_original.shape\n",
    "\n",
    "# Get frame dimensions from the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    raise Exception(\"Could not read frame\")\n",
    "frame_height, frame_width = frame.shape[:2]\n",
    "cap.release()\n",
    "\n",
    "# Define colors for different roles (in BGR format for OpenCV)\n",
    "role_colors = {\n",
    "    \"TEAM A\": (255, 255, 255),  # white\n",
    "    \"TEAM B\": (0, 0, 255),      # red\n",
    "    \"REF\": (0, 255, 255),       # yellow\n",
    "    \"GK\": (0, 0, 0)             # black\n",
    "}\n",
    "\n",
    "# Process each interesting frame\n",
    "for interesting_frame in interesting_frames:\n",
    "    # Create a fresh copy of the pitch for each frame\n",
    "    pitch_img = pitch_img_original.copy()\n",
    "    \n",
    "    # Get the frame with detections\n",
    "    frame_detections = next((item for item in detections if item[\"frame_id\"] == interesting_frame), None)\n",
    "    \n",
    "    if frame_detections:\n",
    "        for detection in frame_detections[\"detections\"]:\n",
    "            coords = detection[\"minimap_coordinates\"]\n",
    "            if coords is None:\n",
    "                continue\n",
    "            x = coords[\"x\"]\n",
    "            y = coords[\"y\"]\n",
    "            x_max = coords[\"x_max\"]\n",
    "            y_max = coords[\"y_max\"]\n",
    "            \n",
    "            # Normalize to pitch dimensions (with scaling factor of 4 applied)\n",
    "            norm_x = int(x / x_max * pitch_width)\n",
    "            norm_y = int(y / y_max * pitch_height)\n",
    "            \n",
    "            # Get player role and determine color\n",
    "            role = detection[\"role\"] if \"role\" in detection else \"UNKNOWN\"\n",
    "            color = role_colors.get(role, (255, 0, 0))  # default to blue\n",
    "            \n",
    "            # Draw player as a circle (increased radius to match the scaling)\n",
    "            cv2.circle(pitch_img, (norm_x, norm_y), 60, color, -1)\n",
    "    \n",
    "    # Save the image\n",
    "    minimap_output_path = f\"{images_for_paper_path}/minimap_{interesting_frame:06d}.png\"\n",
    "    cv2.imwrite(minimap_output_path, pitch_img)\n",
    "    print(f\"Saved minimap visualization for frame {interesting_frame} to {minimap_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 212: Number of clusters found: 6\n",
      "Saved minimap with clusters for frame 212 to data/example/images_for_paper/minimap_decision_critical_zones_000212.png\n",
      "Frame 400: Number of clusters found: 3\n",
      "Saved minimap with clusters for frame 400 to data/example/images_for_paper/minimap_decision_critical_zones_000400.png\n",
      "Frame 460: Number of clusters found: 2\n",
      "Saved minimap with clusters for frame 460 to data/example/images_for_paper/minimap_decision_critical_zones_000460.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "import cv2\n",
    "\n",
    "# Process each interesting frame\n",
    "for interesting_frame in interesting_frames:\n",
    "    # Get the frame with detections\n",
    "    frame_detections = next((item for item in detections if item[\"frame_id\"] == interesting_frame), None)\n",
    "    \n",
    "    if not frame_detections:\n",
    "        continue\n",
    "    \n",
    "    # Extract player positions (excluding referee)\n",
    "    player_positions = []\n",
    "    for detection in frame_detections[\"detections\"]:\n",
    "        coords = detection[\"minimap_coordinates\"]\n",
    "        role = detection[\"role\"] if \"role\" in detection else \"UNKNOWN\"\n",
    "        \n",
    "        # Skip if coordinates are None or if it's a referee\n",
    "        if coords is None or role == \"REF\":\n",
    "            continue\n",
    "        \n",
    "        x = coords[\"x\"]\n",
    "        y = coords[\"y\"]\n",
    "        x_max = coords[\"x_max\"]\n",
    "        y_max = coords[\"y_max\"]\n",
    "        \n",
    "        # Normalize to pitch dimensions (in meters)\n",
    "        norm_x = x / x_max * 105  # 105m is pitch width\n",
    "        norm_y = y / y_max * 68   # 68m is pitch height\n",
    "        \n",
    "        player_positions.append([norm_x, norm_y])\n",
    "\n",
    "    # Convert to numpy array\n",
    "    player_positions = np.array(player_positions)\n",
    "    \n",
    "    # Skip if no player positions\n",
    "    if len(player_positions) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Run DBSCAN to identify clusters\n",
    "    # eps=2 means players within 2 meters of each other\n",
    "    # min_samples=2 means at least 2 players needed to form a cluster\n",
    "    clustering = DBSCAN(eps=4, min_samples=2).fit(player_positions)\n",
    "    \n",
    "    # Get cluster labels\n",
    "    labels = clustering.labels_\n",
    "    \n",
    "    # Number of clusters (excluding noise points with label -1)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    print(f\"Frame {interesting_frame}: Number of clusters found: {n_clusters}\")\n",
    "    \n",
    "    # Load the saved minimap image\n",
    "    minimap_output_path = f\"{images_for_paper_path}/minimap_{interesting_frame:06d}.png\"\n",
    "    minimap_img = cv2.imread(minimap_output_path)\n",
    "    \n",
    "    # For each cluster, draw a circle and the cluster center\n",
    "    for cluster_id in range(n_clusters):\n",
    "        # Get points in this cluster\n",
    "        cluster_points = player_positions[labels == cluster_id]\n",
    "        \n",
    "        # Calculate cluster center\n",
    "        center_x, center_y = np.mean(cluster_points, axis=0)\n",
    "        \n",
    "        # Convert center coordinates back to image pixels\n",
    "        center_x_px = int(center_x / 105 * pitch_width)\n",
    "        center_y_px = int(center_y / 68 * pitch_height)\n",
    "        \n",
    "        # Calculate the radius based on the spread of points in the cluster\n",
    "        if len(cluster_points) > 1:\n",
    "            # Calculate the average distance from points to center\n",
    "            distances = np.sqrt(np.sum((cluster_points - np.array([center_x, center_y]))**2, axis=1))\n",
    "            radius = int(np.max(distances) / 105 * pitch_width) + 80\n",
    "            \n",
    "            # Ensure minimum radius\n",
    "            radius = max(radius, 20)\n",
    "            \n",
    "            # Draw circle around the cluster\n",
    "            cv2.circle(minimap_img, \n",
    "                      (center_x_px, center_y_px), \n",
    "                      radius,\n",
    "                      (0, 255, 0),  # Green color\n",
    "                      8)  # Line thickness\n",
    "        \n",
    "        # Draw cluster center with green dot\n",
    "        cv2.circle(minimap_img, (center_x_px, center_y_px), 20, (0, 255, 0), -1)  # Green dot\n",
    "    \n",
    "    # Save the image with clusters\n",
    "    clusters_output_path = f\"{images_for_paper_path}/minimap_decision_critical_zones_{interesting_frame:06d}.png\"\n",
    "    cv2.imwrite(clusters_output_path, minimap_img)\n",
    "    print(f\"Saved minimap with clusters for frame {interesting_frame} to {clusters_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 212: Angle between duel 1 and referee line: 65.84 degrees\n",
      "Frame 212: Angle between duel 2 and referee line: 35.79 degrees\n",
      "Frame 212: Number of duels found: 5\n",
      "Saved minimap with duels for frame 212 to data/example/images_for_paper/minimap_duels_000212.png\n",
      "Frame 400: Angle between duel 1 and referee line: 48.95 degrees\n",
      "Frame 400: Angle between duel 2 and referee line: 62.97 degrees\n",
      "Frame 400: Number of duels found: 2\n",
      "Saved minimap with duels for frame 400 to data/example/images_for_paper/minimap_duels_000400.png\n",
      "Frame 460: Number of duels found: 0\n",
      "Saved minimap with duels for frame 460 to data/example/images_for_paper/minimap_duels_000460.png\n"
     ]
    }
   ],
   "source": [
    "# Create a figure showing player duels (players from opposing teams within 3 meters)\n",
    "for interesting_frame in interesting_frames:\n",
    "    # Get the frame with detections\n",
    "    frame_detections = next((item for item in detections if item[\"frame_id\"] == interesting_frame), None)\n",
    "    \n",
    "    if not frame_detections:\n",
    "        continue\n",
    "    \n",
    "    # Extract player positions and team information\n",
    "    player_positions = []\n",
    "    player_teams = []\n",
    "    referee_position = None  # Initialize referee position\n",
    "    \n",
    "    for detection in frame_detections[\"detections\"]:\n",
    "        coords = detection[\"minimap_coordinates\"]\n",
    "        role = detection[\"role\"] if \"role\" in detection else \"UNKNOWN\"\n",
    "        \n",
    "        # Store referee position separately\n",
    "        if role == \"REF\" and coords is not None:\n",
    "            x = coords[\"x\"]\n",
    "            y = coords[\"y\"]\n",
    "            x_max = coords[\"x_max\"]\n",
    "            y_max = coords[\"y_max\"]\n",
    "            \n",
    "            # Normalize to pitch dimensions (in meters)\n",
    "            norm_x = x / x_max * 105  # 105m is pitch width\n",
    "            norm_y = y / y_max * 68   # 68m is pitch height\n",
    "            \n",
    "            referee_position = np.array([norm_x, norm_y])\n",
    "            continue\n",
    "            \n",
    "        # Skip if coordinates are None\n",
    "        if coords is None:\n",
    "            continue\n",
    "        \n",
    "        # Determine team based on role\n",
    "        team = 1 if role == \"TEAM A\" else 2 if role == \"TEAM B\" else 0\n",
    "        \n",
    "        x = coords[\"x\"]\n",
    "        y = coords[\"y\"]\n",
    "        x_max = coords[\"x_max\"]\n",
    "        y_max = coords[\"y_max\"]\n",
    "        \n",
    "        # Normalize to pitch dimensions (in meters)\n",
    "        norm_x = x / x_max * 105  # 105m is pitch width\n",
    "        norm_y = y / y_max * 68   # 68m is pitch height\n",
    "        \n",
    "        player_positions.append([norm_x, norm_y])\n",
    "        player_teams.append(team)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    player_positions = np.array(player_positions)\n",
    "    player_teams = np.array(player_teams)\n",
    "    \n",
    "    # Skip if no player positions\n",
    "    if len(player_positions) < 2:\n",
    "        continue\n",
    "    \n",
    "    # Load the saved minimap image\n",
    "    minimap_output_path = f\"{images_for_paper_path}/minimap_{interesting_frame:06d}.png\"\n",
    "    minimap_img = cv2.imread(minimap_output_path)\n",
    "    \n",
    "    # Find duels (players from opposing teams within 3 meters)\n",
    "    duels = []\n",
    "    for i in range(len(player_positions)):\n",
    "        for j in range(i+1, len(player_positions)):\n",
    "            # Check if players are from different teams\n",
    "            if player_teams[i] != player_teams[j]:\n",
    "                # Calculate distance between players\n",
    "                dist = np.sqrt(np.sum((player_positions[i] - player_positions[j])**2))\n",
    "                \n",
    "                # If distance is less than 3 meters, it's a duel\n",
    "                if dist < 3:\n",
    "                    duels.append((i, j))\n",
    "    \n",
    "    # Draw duels on the minimap\n",
    "    for i, j in duels:\n",
    "        # Get positions of the two players\n",
    "        pos1, pos2 = player_positions[i], player_positions[j]\n",
    "        \n",
    "        # Convert coordinates to image pixels\n",
    "        x1_px = int(pos1[0] / 105 * pitch_width)\n",
    "        y1_px = int(pos1[1] / 68 * pitch_height)\n",
    "        x2_px = int(pos2[0] / 105 * pitch_width)\n",
    "        y2_px = int(pos2[1] / 68 * pitch_height)\n",
    "        \n",
    "        # Draw a line connecting the two players\n",
    "        cv2.line(minimap_img, \n",
    "                (x1_px, y1_px), \n",
    "                (x2_px, y2_px), \n",
    "                (0, 255, 0),  # Green color\n",
    "                8)  # Line thickness\n",
    "        \n",
    "        # Draw dots for the players\n",
    "        team1_color = (0, 255, 0)  # Green for team 1\n",
    "        team2_color = (0, 255, 0)  # Green for team 2\n",
    "        \n",
    "        cv2.circle(minimap_img, (x1_px, y1_px), 20, \n",
    "                  team1_color if player_teams[i] == 1 else team2_color, -1)\n",
    "        cv2.circle(minimap_img, (x2_px, y2_px), 20, \n",
    "                  team1_color if player_teams[j] == 1 else team2_color, -1)\n",
    "    \n",
    "    # Draw lines from referee to the two closest duels if referee is present\n",
    "    if referee_position is not None and duels:\n",
    "        # Convert referee position to image pixels\n",
    "        ref_x_px = int(referee_position[0] / 105 * pitch_width)\n",
    "        ref_y_px = int(referee_position[1] / 68 * pitch_height)\n",
    "        \n",
    "        # Draw referee position\n",
    "        cv2.circle(minimap_img, (ref_x_px, ref_y_px), 20, (0, 255, 0), -1)  # Green dot for referee\n",
    "        \n",
    "        # Calculate distances from referee to all duels\n",
    "        duel_distances = []\n",
    "        \n",
    "        for idx, (i, j) in enumerate(duels):\n",
    "            # Calculate midpoint of the duel\n",
    "            duel_midpoint = (player_positions[i] + player_positions[j]) / 2\n",
    "            \n",
    "            # Calculate distance from referee to duel midpoint\n",
    "            dist = np.sqrt(np.sum((referee_position - duel_midpoint)**2))\n",
    "            \n",
    "            duel_distances.append((idx, dist))\n",
    "        \n",
    "        # Sort duels by distance to referee\n",
    "        duel_distances.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Draw lines to the two closest duels if available\n",
    "        for k in range(min(2, len(duel_distances))):\n",
    "            closest_duel_idx = duel_distances[k][0]\n",
    "            i, j = duels[closest_duel_idx]\n",
    "            \n",
    "            # Get midpoint of the duel\n",
    "            duel_midpoint = (player_positions[i] + player_positions[j]) / 2\n",
    "            \n",
    "            # Convert midpoint to image pixels\n",
    "            mid_x_px = int(duel_midpoint[0] / 105 * pitch_width)\n",
    "            mid_y_px = int(duel_midpoint[1] / 68 * pitch_height)\n",
    "            \n",
    "            # Draw a line from referee to duel midpoint\n",
    "            cv2.line(minimap_img, \n",
    "                    (ref_x_px, ref_y_px), \n",
    "                    (mid_x_px, mid_y_px), \n",
    "                    (0, 255, 0),  # Green color\n",
    "                    8,  # Line thickness\n",
    "                    cv2.LINE_AA)  # Anti-aliased line\n",
    "            \n",
    "            # Calculate angle between duel line and referee line\n",
    "            duel_vector = player_positions[j] - player_positions[i]\n",
    "            ref_to_duel_vector = duel_midpoint - referee_position\n",
    "            \n",
    "            # Calculate unit vectors\n",
    "            duel_unit = duel_vector / np.linalg.norm(duel_vector)\n",
    "            ref_unit = ref_to_duel_vector / np.linalg.norm(ref_to_duel_vector)\n",
    "            \n",
    "            # Calculate dot product and angle\n",
    "            dot_product = np.dot(duel_unit, ref_unit)\n",
    "            # Clip to handle floating point errors\n",
    "            dot_product = np.clip(dot_product, -1.0, 1.0)\n",
    "            angle_rad = np.arccos(dot_product)\n",
    "            angle_deg = np.degrees(angle_rad)\n",
    "            \n",
    "            # Always get the acute angle (less than 90 degrees)\n",
    "            if angle_deg > 90:\n",
    "                angle_deg = 180 - angle_deg\n",
    "                \n",
    "            print(f\"Frame {interesting_frame}: Angle between duel {k+1} and referee line: {angle_deg:.2f} degrees\")\n",
    "    \n",
    "    # Save the image with duels\n",
    "    duels_output_path = f\"{images_for_paper_path}/minimap_duels_{interesting_frame:06d}.png\"\n",
    "    cv2.imwrite(duels_output_path, minimap_img)\n",
    "    print(f\"Frame {interesting_frame}: Number of duels found: {len(duels)}\")\n",
    "    print(f\"Saved minimap with duels for frame {interesting_frame} to {duels_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
